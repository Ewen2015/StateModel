# Statistical Models

- Regression Models

- Generalized Regression Models

- Experimental Design

- Mixed Models

- Computational and Graphical Methods 

- Methods in Simulation and Computation

- Survey Statistics

- Nonparametric Methods

- Time Series Analysis

- Multivariate Analysis

- Bayesian Statistics

- Statistical Learning and Data Mining

I will discuss topics listed above according to my understanding. If you have any questions, feel free to leave a comment. I would like learn about your understanding. :smiley:

### 1. Regression Models

Probabilities and mathematical statistics are the foundation of Statistics, clearly. Stand behind these two basical courses are regression analysis. The importance of a good understanding of these two courses can never be overestimated, not only for statistics but machine learning. 

#### Overview

So, what is the start point of it? Or in orther words, what issues does regression analysis intend to solve?

A short answer is **the relationship between covariates (X) and response(Y)**. 

In terms of classical statistics, regression means relationship, seriously. You can check a sad <a href="http://blog.minitab.com/blog/statistics-and-quality-data-analysis/so-why-is-it-called-regression-anyway">story</a> behind the naming of regression.

I guess this is one of the cores of all studies -- the **relationship**. For statistics, we intend to analyze the relationship reflected by data. Furthermore, I would like to give you a big picture of how it is organized.

|         |           |Covariates|||
|---------|-----------|---|---|---|
| | |Continuous | Catigorical | Mixed |
|Response |Continuous |Normal Regression | ANOVA | Multiple Linear Regression|
|Response |Catigorical|Logistic Regression; Generalized Linear Model | Contigency Table | Generalized Linear Regression|

In the perspective of machine learning (statistcal learning, data mining), all above are **supervised learning**, for given response and predictors. In addition, similar problems have different names between classical statistics and machine learning: **regression and classification**. There are more efficient tools in statistical learning, in terms of ablity of prediction. However, they may lose the convenience of interpretation. 

As mentioned above, there is a **trade-off** between **predictability and interpretability**. This is one of the big difference between the original intention fo classical statistics and machine learing. The former one is created to explain the relationships, while the latter one is designed for prediction. 

#### Linear Regression Models

Come back to regression models, leaving aside models like **Analysis of Variance (ANOVA) and contigency table of counts**, what is the insight of the **linear regression models**? As we known, there are plenty of linear regression models - simple linear regression, multiple regression, logistic regression, Poisson regression, you name it. But what are they in common, or can we put all of them in a nutshell?

Yes, we can. 

That's the idea fo **General Linear Regression (GLR): the mean (or transformed mean) of a random variable follows a linear model**. This sentence includes the three components of a GLR: 

|Link Function |Random Component |Systematic Component |
|---|---|---|
|mean (or transformed mean)|a random variable|a linear model|

The source of this idea comes from the **distributions** of response. As mentioned, probabilities and mathematical statistics are the foundation of Statistics. 

|Distribution |Random Component | Link Function | Systematic Component |
|---|---|---|---|
|Normal      | y ~ Normal      | link(u) = u        | BX |
|Binomial    | y ~ Binomial    | link(u) = logit(u) | BX |
|Multinomial | y ~ Multinomial | link(u) = logit(u) | BX |
|Poisson     | y ~ Poisson     | link(u) = log(u)   | BX |

<a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function">Wikipedia</a> provides more link functions of different distributions.

The above is the big picture of generalized lieanr regression models. I did not go into details in them, but it does not mean no details there. Actually, there are much more, including model checking (can we simplify our model? or is our mdoel fit adequatly?), model diagnostics (outliers, systematic structure, or overdispersion), and one model in different scenario being different. We restric our discussion to a big picture of regression model, and wont cover these staff here.

#### Other Regressiom Models

**Analysis of variance (ANOVA)** is a collection of statistical models used to analyze the **differences among group means and their associated procedures** (<a href="https://en.wikipedia.org/wiki/Analysis_of_variance">Wikipedia</a>). This part is frequently used in **experimental design**, for whom covariates are usually categorical (factor with levels). 

A **contingency table** is a type of table in a matrix format that displays the (multivariate) **frequency distribution** of the variables. People usually use it to test **homogenity and independence**. A classical example is <a href="https://en.wikipedia.org/wiki/Lady_tasting_tea">Lady Tasting Tea</a>. We use <a href="https://en.wikipedia.org/wiki/Fisher%27s_exact_test">Fisher's Exact Test</a> to analyze its independence.

||Lady: Milk First |Lady: Tea First|
|---|---|---|
|Truth: Milk First| 3 | 1 |
|Truth: Tea Firts | 1 | 3 |

The analysis of contingency tables depends on the probability distribution of count data, which, however, depends on the study design. We provide several types of study, but will not go into details. 

|Control on? |Study Name |Study Type |Data Avaliable? |Probability Distribution |
|---|---|---|---|---|
|None or n |Cross-Sectional |Observational |Cross-Sectional |Poisson; Multinomial |
|X total |Cohort Study if X is self-selected |Observational |Prospective |Indep. Multinomial across rows |
|X total |Clinical Study if X is randomly assigned |Experimental |Prospective |Non-indep. Multinomial across columns |
|Y total or all fixed |Case Control |Observational |Retrospective |Hypergeometric |

Some common tools used to test independence are Chi-Square Test, Pearson's Chi-Square test, and Likelihood Ratio Test. In addition, we can develop the two-way contingency tables to three-way one, but will not discuss it here. 

May 29, 2016

Fort Collins

--- 

### 2. Time Series

The definition of time series is clear and intuitive, but less insight: a sequence of measurements of the same variable collected over time. Yes, we know a time series is a sequence of measurements indexed by discrete time, but can you use some statistical language tell us what does it concern about? As we underlined before, probabilities and mathematical statistics locate at important position of the whole statistics. We could and should think in a statistical way. 

#### Overview

The core of analysis of time series is **dependence**. Being indexed by time is just a superficial attribute of time series, the very underlying feature of it is the dependence of that series. Instead of analyze the relationship of covariates and response, we care more about the behavior of one vairable over time. This is a take-home message of this article. 

#### Autoregressive Models

Before I go into detials of specific time series models, please follow me to do a thinking exercise. I have explained that the difference between time series and regression models we discussed in last section is denpendence. So, my question is that where can the dependence come from? 

It is no difficult to find one possibility -- the dependence comes from a previous measurement or p previous measurements of that variable. Yes, you have already get started your journey of time series. This case is a very common one in nature and ecomonic, like quake data and Google stock data. We call them **Autoregressive Models (AR)**. In general, we express this dependence as following equation:

x_t = \phi x_{t-1} + w_t

where \{w_t\} are **uncorrelated** random variables with **mean 0 and finite variance**, and called **white noise**. If white noises are independent, they are called iid noise, which are not required here. More details about independence and uncorrelated will be discussed later. 

According to different values of \phi, the autoregressive models (AR) have specific names.

|\phi|AM Type |
|---|---|
|\phi = 0| White Noise|
|absolute \phi < 1| Stationary AM|
|\phi = 1| Random Walk|
|\phi > 1| Explosion|

#### Moving Average Models

Let's continue our thinking exercise. What if the latter measurement is uncorrelated to the former one? Where can the dependence come from? 

Inspired by the AR models, it is reasonable to guess that another possibility comes from a linear combination of q previous white noises. We call this kind of models as **Moving Average (MA)** models. Based on similar idea of classificatin as AR models, moving average (MA) can be grouped into several types. 

|Weights of q previous white noises |MA Type |
|---|---|
|Equally weighted|Simple Moving Average|
|Further data gets the less weight|Weighted Moving Average|
|Further data gets the less weight exponentially| Exponential Moving Average|

So far we have discussed two bisic time series models: AR and MA. It is naturaly to consider combining these two models as one, then we get ARMA models. We usually write an ARMA model as ARMA(p, q), where p indicates p previous measurements and q stands for q previouse white noises. 

#### ARIMA and SARIMA

Sometimes you will encounter with ARIMA(p, d, q) or more complicated SARIMA(p, d, q, P, D, Q). I indicates differencing d times, for a stationary model. S stands for seanonal model, just a bigger version of the original one. I would like to treat differencing as a kind of data preprocessing. Besides, seasonal models can be discovered in the stage of Exploratory Data Analysis (EDA), and be fulfilled by common software like R. More details will not be covered here.

#### Autoregressive Conditional Heteroskedastic Models

We have find two source of dependence of time series, what else? The requirement of uncorrelated but not independent white noises foreshadows the third possibility of dependence. When variance of measurements are constant, but conditional variance are heteroskedastic, we refer it as Autoregressive Conditional Heteroskedastic Models (ARCH). More detials can be found in <a href="https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity">Wikipedia</a>.

May 30, 2016

Fort Collins

---

#### 3. Experimental Design and Mixed Models

If we regard the regression analysis and time series analysis as observational studies, we now will take a look at another type of study -- experiments. The difference between these two kinds of studies is pretty straightforward: people can control variables in an experiment, while only nature conditions can be used in an observational study. As a result, cause-and-effect relationship may be found through experiments, while you can only find association relationship by observational studies. 

















